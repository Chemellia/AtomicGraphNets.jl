<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Basic Graph Theory · AtomicGraphNets.jl</title><link rel="canonical" href="https://aced-differentiate.github.io/AtomicGraphNets.jl/stable/graph_theory/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">AtomicGraphNets.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Basic Graph Theory</a><ul class="internal"><li><a class="tocitem" href="#What-is-a-graph?"><span>What is a graph?</span></a></li><li><a class="tocitem" href="#Adjacency-matrix"><span>Adjacency matrix</span></a></li><li><a class="tocitem" href="#Degree-matrix"><span>Degree matrix</span></a></li><li><a class="tocitem" href="#Graph-Laplacian"><span>Graph Laplacian</span></a></li><li><a class="tocitem" href="#Remarks"><span>Remarks</span></a></li></ul></li><li><a class="tocitem" href="../gcnns/">GCNNs</a></li><li><a class="tocitem" href="../comparison/">Comparison with cgcnn.py</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/example_1/">Example 1</a></li><li><a class="tocitem" href="../examples/example_2/">Example 2</a></li></ul></li><li><span class="tocitem">Functions</span><ul><li><a class="tocitem" href="../functions/layers/">Layers</a></li><li><a class="tocitem" href="../functions/models/">Models</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Basic Graph Theory</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Basic Graph Theory</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/aced-differentiate/AtomicGraphNets.jl/blob/master/docs/src/graph_theory.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Basic-Graph-Theory"><a class="docs-heading-anchor" href="#Basic-Graph-Theory">Basic Graph Theory</a><a id="Basic-Graph-Theory-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Graph-Theory" title="Permalink"></a></h1><p>A practical jumpstart, (and not an exhaustive resource), that introduces the bare minimum graph theory concepts to understand the math behind graph convolution.</p><h2 id="What-is-a-graph?"><a class="docs-heading-anchor" href="#What-is-a-graph?">What is a graph?</a><a id="What-is-a-graph?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-graph?" title="Permalink"></a></h2><p>A graph is an object described by a set of <strong>nodes</strong> and a set of <strong>edges</strong> that connect them:</p><p><img src="../assets/images/basic_graph.png" alt="Graph"/></p><p>This is a useful data structure for all kinds of things, such as <a href="https://en.wikipedia.org/wiki/Seven_Bridges_of_K%C3%B6nigsberg">bridges in German cities</a> (the historical origin of graph theory), <a href="https://en.wikipedia.org/wiki/Social_network_analysis">social networks</a> (in a more modern context), and many other applications. In this package, we&#39;re interested in using graphs to represent structures made of atoms such as crystals and molecules. Think of those ball-and-stick models of atoms and molecules from high school chemistry class!</p><p>In order to analyze graphs in a computer (for machine learning applications or otherwise), the first step is to assign an index to each node. It will turn out that for our purposes, it doesn&#39;t matter which node gets which label, which is very convenient!</p><p><img src="../assets/images/graph_nodelabels.png" alt="Graph with node labels"/></p><p>Note that it is possible to have an edge with both ends connected to the same node (as shown in node 1 above), this is referred to as a <strong>self-loop</strong>.</p><p>The graph structure is represented on a computer using several types of matrices, defined below...</p><h2 id="Adjacency-matrix"><a class="docs-heading-anchor" href="#Adjacency-matrix">Adjacency matrix</a><a id="Adjacency-matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Adjacency-matrix" title="Permalink"></a></h2><p>Arguably, the most fundamental matrix representing a graph (because it has all the information required to construct the other two types we will introduce) is the <strong>adjacency matrix</strong>. An adjacency matrix is a square matrix of dimensions <span>$n \times n$</span>, where <span>$n$</span> is the number of nodes in the graph being represented. The entry at index <em>(i,j)</em> in the adjacency matrix of an unweighted, undirected graph is 1 if node <em>i</em> and <em>j</em> are connected and zero otherwise.</p><p>For our sample graph above, the adjacency matrix is:</p><p class="math-container">\[A = \begin{bmatrix}
1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}\]</p><p>A few variations exist...</p><ul><li><p><strong>Weighted Graphs</strong>: Weighted graphs are graphs whose adjacency matrix representations have a continuum of values, rather than only <span>$0&#39;s$</span> or <span>$1&#39;s$</span>. <code>AtomGraph</code> objects as defined in <a href="https://github.com/aced-differentiate/ChemistryFeaturization.jl">ChemistryFeaturization.jl</a> are weighted graphs, and the weights are scaled (without loss of generality) such that the largest weight is equal to <span>$1$</span>.<br/>For more on how weights are calculated, see the <a href="thazhemadam.github.io/ChemistryFeaturization.jl/dev">ChemistryFeaturization.jl documentation</a>.</p></li><li><p><strong>Directed Graphs</strong>: A directed graph&#39;s adjacency matrix representation can be an asymmetric matrix. In other words, the edges of the graph have directionality. Atomic graphs are generally undirected.</p></li></ul><h2 id="Degree-matrix"><a class="docs-heading-anchor" href="#Degree-matrix">Degree matrix</a><a id="Degree-matrix-1"></a><a class="docs-heading-anchor-permalink" href="#Degree-matrix" title="Permalink"></a></h2><p>The degree matrix of a graph is a diagonal matrix that describes how many edge terminations are at each node. For our sample graph, the degree matrix is given by:</p><p class="math-container">\[D = \begin{bmatrix}
4 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 3 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\end{bmatrix}\]</p><p>In a weighted graph, the degree matrix is constructed by summing the weights rather than just counting the nonzero entries in that row/column of the adjacency matrix.</p><h2 id="Graph-Laplacian"><a class="docs-heading-anchor" href="#Graph-Laplacian">Graph Laplacian</a><a id="Graph-Laplacian-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-Laplacian" title="Permalink"></a></h2><p>The Laplacian matrix (also called the graph Laplacian) is defined as the difference of the degree and adjacency matrices.</p><p class="math-container">\[L = D - A = \begin{bmatrix}
3 &amp; -1 &amp; -1 &amp; 0 &amp; 0\\
-1 &amp; 2 &amp; 0 &amp; -1 &amp; 0\\
-1 &amp; 0 &amp; 2 &amp; -1 &amp; 0\\
0 &amp; -1 &amp; -1 &amp; 3 &amp; -1\\
0 &amp; 0 &amp; 0 &amp; -1 &amp; 1\end{bmatrix}\]</p><p>As the name suggests, this matrix is closely tied to the differential operator that comes up in, for example, the diffusion equation. The graph laplacian matrix as an operator is, in fact, <em>diffusional</em> in nature. To get a better sense of this, let&#39;s define a simple graph signal, just a single number at each node of the graph, and we&#39;ll start with a &quot;delta spike&quot; on node 5 and zeros everywhere else. Watch what happens when we operate on that signal with the Laplacian a couple times...</p><p><img src="../assets/images/conv.gif" alt="Laplacian as an operator"/></p><p>For a deeper dive into how graph Laplacian works, check <a href="https://samidavies.wordpress.com/2016/09/20/whats-up-with-the-graph-laplacian/">this</a> out. But at this point, the more important thing, is to know that this in the most generalized sense, is how we graph convolution is performed.</p><p>In real applications, the &quot;graph signal&quot; is a feature matrix rather than a vector, as we&#39;ll generally have a vector of features for each node, and these get stacked to form the feature matrix. You can convince yourself with some basic linear algebra that the result of this is the same as if you applied the convolutional operation (i.e., multiplying by the Laplacian) to a bunch of vectors individually and stacked the results.</p><h2 id="Remarks"><a class="docs-heading-anchor" href="#Remarks">Remarks</a><a id="Remarks-1"></a><a class="docs-heading-anchor-permalink" href="#Remarks" title="Permalink"></a></h2><p>A concise analysis that goes off on a tangent to discuss few facets that might be useful to understand before getting into graph convolution.</p><h3 id="Equivariance-to-node-indexing"><a class="docs-heading-anchor" href="#Equivariance-to-node-indexing">Equivariance to node indexing</a><a id="Equivariance-to-node-indexing-1"></a><a class="docs-heading-anchor-permalink" href="#Equivariance-to-node-indexing" title="Permalink"></a></h3><p>The results of the convolution, i.e., the set of feature vectors at each node, are independent of the order in which the nodes are indexed, and identified. That is, if we relabel nodes with different indices, the adjacency, degree, Laplacian, and feature matrices are all re-calculated accordingly, in just the right way for everything to work out. Try out the example above with different node labels, to see this in practice.</p><h3 id="Normalized-Laplacian"><a class="docs-heading-anchor" href="#Normalized-Laplacian">Normalized Laplacian</a><a id="Normalized-Laplacian-1"></a><a class="docs-heading-anchor-permalink" href="#Normalized-Laplacian" title="Permalink"></a></h3><p>A not-so-nice aspect of the graph Laplacian as defined here, is that the magnitude of the graph signal can change upon repeated convolutions. In practice, the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix#Symmetric_normalized_Laplacian_2">normalized Laplacian</a> is used, which is computed using the inverse square root degree matrix. This helps regulate things better.</p><h3 id="How-is-this-related-to-image-convolution?"><a class="docs-heading-anchor" href="#How-is-this-related-to-image-convolution?">How is this related to image convolution?</a><a id="How-is-this-related-to-image-convolution?-1"></a><a class="docs-heading-anchor-permalink" href="#How-is-this-related-to-image-convolution?" title="Permalink"></a></h3><p>For those familiar with image convolution (for those unfamiliar, there&#39;s a nice interactive tutorial <a href="https://setosa.io/ev/image-kernels/">here</a>), it might not always be immediately obvious how this procedure is related. But, it turns out this is <em>exactly</em> the same thing. In typical image convolution, we&#39;ve imposed a very rigid structure on the underlying graph namely that every node (pixel) has exactly the same sort of local structure (Cartesian neighbors). Graph convolution is more flexible, and allows for a more generalized notion of which &quot;pixels&quot; are &quot;neighbors.&quot;</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../gcnns/">GCNNs »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 22 March 2021 15:08">Monday 22 March 2021</span>. Using Julia version 1.5.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
