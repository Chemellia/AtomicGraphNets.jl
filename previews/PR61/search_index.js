var documenterSearchIndex = {"docs":
[{"location":"examples/example_2/#QM9-Dataset","page":"Example 2","title":"QM9 Dataset","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"In this example, we will create and train a neural network based on the architecture as introduced in this paper, using the QM9 dataset.","category":"page"},{"location":"examples/example_2/#Train-the-Network","page":"Example 2","title":"Train the Network","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"Run the qm9.jl script in your Julia environment and see what happens!","category":"page"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"Note: The .xyz files provided within the QM9 dataset are not parsable directly by ASE. So, the last couple lines need to be removed, which is easy enough to be done using a simple script. For convenience, and demonstration purposes, a small set of the modified .xyz files have been made available here.","category":"page"},{"location":"examples/example_2/#Remarks","page":"Example 2","title":"Remarks","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"It is well worth noting that the actual model performance on QM9 is not that great since we're currently not encoding a variety of important features for organic molecules.\nThis is provided mainly to show the processing of a different dataset and demonstrate batch processing capabilities.","category":"page"},{"location":"comparison/#Comparison-of-AtomicGraphNets.jl-and-cgcnn.py","page":"Comparison with cgcnn.py","title":"Comparison of AtomicGraphNets.jl and cgcnn.py","text":"","category":"section"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"AtomicGraphNets.jl provides a model-builder for the typical GCNN architecture called Xie_model; in reference to Tian Xie, the original developer of cgcnn.py.\nHowever, there are some differences between how AtomicGraphNets models and those ones work, particularly with respect to the convolutional operation performed.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The cgcnn.py package was the first major package to implement atomic graph convolutional networks.\nHowever, the \"convolutional\" operation they use, while qualitatively similar, is not convolution by the strict definition involving the graph Laplacian. In their package, they introduce two such operations.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The first operation is expressed as follows.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nv^(t+1)_i = g (sumlimits_jkv^(t)_j bigoplus u_(ij)_k)W^(t)_c + v^(t)_iW^(t)_s + b^(t)\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"Here, v, u represent node features and edge features respectively, and i, j, k index nodes, neighbors of nodes, and edge multiplicities respectively. Further, bigoplus indicates concatenation, and g is an activation function.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"Note that such an operation, which does not make use of the graph Laplacian, requires explicit computation of neighbor lists for every node, and that the convolutional weight matrix is of very large dimension due to the concatenation step.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The original CGCNN paper explores the following slightly more complicated operation that resulted in better performance.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nv^(t+1)_i = v^(t)_i + sumlimits_jksigma(z^(t)_(ij)_kW^(t)_f + b^(t)_f) bigodot g(z^(t)_(ij)_kW^(t)_s + b^(t)_s))\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"where z is a concatenation of neighbor features and edge features, and bigodot indicates element-wise multiplication.\nThis operation entails yet more trainable parameters, and neither operation is particularly performant because the concatenation operation must be done at each step of the forward pass.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The operation implemented in AtomicGraphnets is as follows.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nX^(t+1) = n_zgW^(t)_c cdot X^(t) cdot L + W^(t)_s cdot X^(t) + B^(t)\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"where X is a feature matrix constructed by stacking feature vectors, B is a bias matrix (stacked identical copies of the per-feature bias vector) and n_z is the z-score normalization (or regularized normalization operation), which we have found to improve stability.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"In addition, since the graph Laplacian need only be computed once (and is in fact stored as part of the AtomGraph type), the forward pass is much more computationally efficient.\nSince no concatenation occurs, weight matrices are also smaller, meaning the model has fewer trainable parameters, and no sacrifice in accuracy that we have been able to observe, indicating comparable expressivity.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"It is worth noting that one advantage of the cgcnn.py approach is that it allows for explicitly enumerating edge features.\nIn the current version of AtomicGraphNets, the only features of graph edges are their weights.\nConvolutional operations that allow for edge features are under consideration for future versions.","category":"page"},{"location":"examples/example_1/#Materials-Project-Database","page":"Example 1","title":"Materials Project Database","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"In this example, we will create and train a neural network on the property formation_energy_per_atom based on the architecture as introduced in this paper, using the Materials Project database.","category":"page"},{"location":"examples/example_1/#.-Set-up-the-Dataset","page":"Example 1","title":"1. Set up the Dataset","text":"","category":"section"},{"location":"examples/example_1/#a.-Set-up-required-dependencies","page":"Example 1","title":"a. Set up required dependencies","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Create an API key with Materials Project to download the training dataset for this example.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Python packages (primarily pymatgen) are also required as dependencies. The easiest way to install these in a new environment, is using Conda, by running the following commands.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"conda create -name agn_example1\nconda activate agn_example1\nconda install -c conda-forge pymatgen=2022.0.4","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Note:  Alternatively, to install dependencies in an existing environment, you can skip the first step, activate your environment, and directly install pymatgen=2022.0.4.","category":"page"},{"location":"examples/example_1/#b.-Download-the-Data","page":"Example 1","title":"b. Download the Data","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"To download the structures, simply run","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"python download_data.py \"YOUR_API_KEY\"\nwhere, \"YOUR_API_KEY\" is replaced with your actual API key obtained from the Materials Project database.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"This downloads the data required to train the models into the directory example1/data.","category":"page"},{"location":"examples/example_1/#.-Train-the-Network","page":"Example 1","title":"2. Train the Network","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Now that the dataset has been set up, run the formation_energy.jl file in your Julia environment and see what happens!","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Feel free to peruse and play with other options defined at the top of the file as well and see how it impacts the results!\nIn particular, the defaults have been set to only have a dataset of size 100 so that the base case will run quickly, but feel free to try more data to see how much better the results get.","category":"page"},{"location":"examples/example_1/#Note","page":"Example 1","title":"Note","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"By default, the property chosen for training is formation energy per atom. If you want to train a different property, replace it with its pymatgen string in the appropriate line. This change must be reflected in both download.py and example.jl.\nThere may be some CIF parsing warnings from pymatgen that show up, but these shouldn't affect things and can be safely ignored.","category":"page"},{"location":"graph_theory/#Basic-Graph-Theory","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A practical jumpstart, (and not an exhaustive resource), that introduces the bare minimum graph theory concepts to understand the math behind graph convolution.","category":"page"},{"location":"graph_theory/#What-is-a-graph?","page":"Basic Graph Theory","title":"What is a graph?","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A graph is structure that is open to multiple specialized interpretations, based on the domain in whose context it is considered. However, all these interpretations of a graph's definition can be essentially summarised to define a graph as a structure that represents a set of nodes connected by edges such that there exists some relation between the connected nodes.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"In this package, we're interested in using graphs to represent structures made of atoms such as crystals and molecules. Think of those ball-and-stick models of atoms and molecules from high school chemistry class!","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"There are multiple methods, and data structures using which graphs can be represented in a machine understandable form. Out of these, the method that is of most interest to us, is using matrices, known popularly as the Adjacency Matrix notation.","category":"page"},{"location":"graph_theory/#Adjacency-matrix","page":"Basic Graph Theory","title":"Adjacency matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"An adjacency matrix is a square matrix of dimensions n times n, where n is the number of nodes in the graph being represented.\nThe elements of the matrix indicate whether pairs of vertices (to which the indices in the matrix correspond) are adjacent or not in the graph. Adjacency matrices corresponding to unweighted graphs are comprised solely of 0s and 1s.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Weighted Graphs\nWeighted graphs are graphs whose adjacency matrix representations have a continuum of values, rather than only 0s or 1s.\nAtomGraph objects as defined in ChemistryFeaturization.jl are weighted graphs, and the weights are scaled (without loss of generality) such that the largest weight is equal to 1.\nFor more on how weights are calculated, see the ChemistryFeaturization.jl documentation.\nDirected Graphs\n A directed graph's adjacency matrix representation is always an asymmetric matrix.\nNote: While generally all the atomic graphs we work with can be represented as a symmetric matrix, this need not be true in all cases.","category":"page"},{"location":"graph_theory/#Degree-matrix","page":"Basic Graph Theory","title":"Degree matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The degree matrix of a graph is a diagonal matrix that describes how many edge terminations are at each node.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Weighted Graphs\nIn a weighted graph, the degree matrix is constructed by summing the weights rather than just counting the nonzero entries in that row/column   of the adjacency matrix.","category":"page"},{"location":"graph_theory/#Graph-Laplacian","page":"Basic Graph Theory","title":"Graph Laplacian","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The Laplacian matrix (also called the graph Laplacian) is defined as the difference of the degree and adjacency matrices.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"As the name suggests, this matrix is closely tied to the differential operator that comes up in, for example, the diffusion equation. The graph laplacian matrix as an operator is, in fact, diffusional in nature. To get a better sense of this, an illustration of the same can be found in the example below.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"For a deeper dive into how graph Laplacian works, check this out. But at this point, the more important thing, is to know that this in the most generalized sense, is how we graph convolution is performed.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"In real applications, the \"graph signal\" is a feature matrix rather than a vector, as we'll generally have a vector of features for each node, and these get stacked to form the feature matrix. You can convince yourself with some basic linear algebra that the result of this is the same as if you applied the convolutional operation (i.e., multiplying by the Laplacian) to a bunch of vectors individually and stacked the results.","category":"page"},{"location":"graph_theory/#Example","page":"Basic Graph Theory","title":"Example","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Now, that we have defined all the fundamentals we need to know, let us consider an example to illustrate, and get a better understanding of the same.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Consider the following graph, G.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"(Image: Example Graph)","category":"page"},{"location":"graph_theory/#Adjacency-Matrix","page":"Basic Graph Theory","title":"Adjacency Matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Let A be the adjacency matrix for G.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A = beginbmatrix 1  1  1  0  0 1  0  0  1  01  0  0  1  00  1  1  0  10  0  0  1  0endbmatrix","category":"page"},{"location":"graph_theory/#Degree-Matrix","page":"Basic Graph Theory","title":"Degree Matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The degree matrix, D for G, is as follows.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"D = beginbmatrix 4  0  0  0  0 0  2  0  0  00  0  2  0  00  0  0  3  00  0  0  0  1endbmatrix","category":"page"},{"location":"graph_theory/#Laplacian-Matrix","page":"Basic Graph Theory","title":"Laplacian Matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Let L be the graph Laplacian.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"L = D - A = beginbmatrix 3  -1  -1  0  0 -1  2  0  -1  0-1  0  2  -1  00  -1  -1  3  -10  0  0  -1  1endbmatrix","category":"page"},{"location":"graph_theory/#Graph-Laplacian-as-an-operator","page":"Basic Graph Theory","title":"Graph Laplacian as an operator","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The Laplacian matrix as an operator is diffusional in nature.\nLet's define a simple graph signal, just a single number at each node of the graph, and we'll start with a \"delta spike\" on node 5 and 0s everywhere else.\nWatch, and infer what happens when we operate on that signal with the Laplacian a couple times. (Image: Laplacian as an operator)","category":"page"},{"location":"graph_theory/#Analysis","page":"Basic Graph Theory","title":"Analysis","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A concise analysis that goes off on a tangent to discuss few facets that might be useful to understand before getting into graph convolution.","category":"page"},{"location":"graph_theory/#Equivariance-to-node-indexing","page":"Basic Graph Theory","title":"Equivariance to node indexing","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The results of the convolution, i.e., the set of feature vectors at each node, are independent of the order in which the nodes are indexed, and identified.\nThis means to say that if and when the order of the column vectors is changed, the adjacency, degree, Laplacian, and feature matrices are all re-calculated accordingly, in just the right way for everything to work out. Try out the example above with different node labels, to see this in practice.","category":"page"},{"location":"graph_theory/#Normalized-Laplacian","page":"Basic Graph Theory","title":"Normalized Laplacian","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A not-so-nice aspect of the graph Laplacian as defined here, is that the magnitude of the graph signal can change upon repeated convolutions. In practice, the normalized Laplacian is used, which is computed using the inverse square root degree matrix. This helps regulate things better.","category":"page"},{"location":"graph_theory/#How-is-this-related-to-image-convolution?","page":"Basic Graph Theory","title":"How is this related to image convolution?","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"For those familiar with image convolution (for those unfamiliar, there's a nice interactive tutorial here), it might not always be immediately obvious how this procedure is related.\nBut, it turns out this is exactly the same thing. In typical image convolution, we've imposed a very rigid structure on the underlying graph namely that every node (pixel) has exactly the same sort of local structure (Cartesian neighbors).\nGraph convolution is more flexible, and allows for a more generalized notion of which \"pixels\" are \"neighbors.\"","category":"page"},{"location":"#AtomicGraphNets.jl","page":"Home","title":"AtomicGraphNets.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Graph based models for machine learning on atomic systems!","category":"page"},{"location":"","page":"Home","title":"Home","text":"AtomicGraphNets implements a variety of graph-based methods, such as Crystal Graph Convolutional Neural Nets; in Julia.\nIt makes use of the Flux ecosystem for model building, the JuliaGraphs ecosystem for graph representation, and ChemistryFeaturization for building, featurizing, and visualizing the graphs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in development as part of the ACED project, funded by ARPA-E DIFFERENTIATE and coordinated by Carnegie Mellon University, in collaboration with Julia Computing, Citrine Informatics, and MIT.","category":"page"},{"location":"gcnns/#Graph-Convolutional-Neural-Networks","page":"GCNNs","title":"Graph Convolutional Neural Networks","text":"","category":"section"},{"location":"gcnns/#How-does-AI-learn-structures?","page":"GCNNs","title":"How does AI learn structures?","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"GCNNs have proven to be one of the best ways using which AI can learn structures represented as graphs.","category":"page"},{"location":"gcnns/#Why-Graphs?","page":"GCNNs","title":"Why Graphs?","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"Graphs provide us with a way to mathematically and explicitly represent complex information; and that includes atomic structure of molecules as well.","category":"page"},{"location":"gcnns/#Why-CNNs?","page":"GCNNs","title":"Why CNNs?","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"CNNs have fixed parameters. This allows us to observe a low memory footprint and lesser computational cost, relative to other deep learning methods.\nCNNs use a local kernel, which lets us build heirarchies of information as per requirement.\nCNNs allow us to retain spatial invariance properties of our data. This is especially beneficial while dealing with information that can be represented as geometric structures.","category":"page"},{"location":"gcnns/#Why-Graphs-CNNs?","page":"GCNNs","title":"Why Graphs + CNNs?","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"Machine Learning today is typically constrained to dealing with data represented using what can be understood to be \"1-dimensional, regular, and uniform data structures\"; and identifying some properties (typically euclidean-based) represented that the machine learning algorithm can exploit.\nHowever, if presented with data represented in a structure (such as graphs) that doesn't quite fit this category, then typically the first step followed is to steamroll the data (using techniques such as dimensionality reduction) in an attempt to flatten it out, and essentially vectorize it.\nHowever, in doing so, vital spatial aspects of the data, and structural information related to problem gets discarded.\nThis can lead to wrong relational information being learnt by the model; and can also sometimes principally prove to be counterproductive.","category":"page"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"For this reason, we try and employ Graph Convolutional Neural Networks.\nGraphs are an excellent way for representing relationships and models (such as a molecule).\nWith graphs, we try to ensure that along with the node's properties, aggregated information regarding the node's neighbourhood is also represented. When used in conjunction with CNNs we are presented with a way to combine this information (using some mathematical operations and functions), and to convert it into a higher level representation that can be more useful to the model.\n","category":"page"}]
}
