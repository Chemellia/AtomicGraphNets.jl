var documenterSearchIndex = {"docs":
[{"location":"functions/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"functions/layers/","page":"Layers","title":"Layers","text":"Pages=[\"layers.md\"]","category":"page"},{"location":"functions/layers/","page":"Layers","title":"Layers","text":"AtomicGraphNets.layers.AGNConv\nAtomicGraphNets.layers.AGNPool","category":"page"},{"location":"functions/layers/#AtomicGraphNets.layers.AGNConv","page":"Layers","title":"AtomicGraphNets.layers.AGNConv","text":"AGNConv(in=>out)\n\nAtomic graph convolutional layer. Almost identical to GCNConv from GeometricFlux but adapted to be most similar to Tian's original AGNN structure, so explicitly has self and convolutional weights separately.\n\nFields\n\nselfweight::Array{T,2}: weights applied to features at a node\nconvweight::Array{T,2}: convolutional weights\nbias::Array{T,2}: additive bias (second dimension is always 1 because only learnable per-feature, not per-node)\nσ::F: activation function (will be applied before reg_norm to outputs), defaults to softplus\n\nArguments\n\nin::Integer: the dimension of input features.\nout::Integer: the dimension of output features.\nσ=softplus: activation function\ninitW=glorot_uniform: initialization function for weights\ninitb=zeros: initialization function for biases\n\n\n\n\n\n","category":"type"},{"location":"functions/layers/#AtomicGraphNets.layers.AGNPool","page":"Layers","title":"AtomicGraphNets.layers.AGNPool","text":"Custom pooling layer that outputs a fixed-length feature vector irrespective of input dimensions, for consistent handling of different-sized graphs feeding to fully-connected dense layers afterwards. Adapted from Flux's MeanPool.\n\nIt accepts a pooling width and will adjust stride and/or padding such that the output vector length is correct.\n\n\n\n\n\n","category":"type"},{"location":"examples/example_2/#QM9-Dataset","page":"Example 2","title":"QM9 Dataset","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"In this example, we will create and train a neural network based on the architecture as introduced in this paper, using the QM9 dataset.","category":"page"},{"location":"examples/example_2/#Train-the-Network","page":"Example 2","title":"Train the Network","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"Run the qm9.jl script in your Julia environment and see what happens!","category":"page"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"Note: The .xyz files provided within the QM9 dataset are not parsable directly by ASE. So, the last couple lines need to be removed, which is easy enough to be done using a simple script. For convenience, and demonstration purposes, a small set of the modified .xyz files have been made available here.","category":"page"},{"location":"examples/example_2/#Remarks","page":"Example 2","title":"Remarks","text":"","category":"section"},{"location":"examples/example_2/","page":"Example 2","title":"Example 2","text":"It is well worth noting that the actual model performance on QM9 is not that great since we're currently not encoding a variety of important features for organic molecules.\nThis is provided mainly to show the processing of a different dataset and demonstrate batch processing capabilities.","category":"page"},{"location":"functions/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"functions/models/","page":"Models","title":"Models","text":"Pages=[\"models.md\"]","category":"page"},{"location":"functions/models/","page":"Models","title":"Models","text":"Xie_model","category":"page"},{"location":"functions/models/#AtomicGraphNets.Xie_model","page":"Models","title":"AtomicGraphNets.Xie_model","text":"Build a model of the architecture introduced in the Xie and Grossman 2018 paper: https://arxiv.org/abs/1710.10324\n\nInput to the resulting model is a FeaturedGraph with feature matrix with input_feature_length rows and one column for each node in the input graph.\n\nNetwork has convolution layers, then pooling to some fixed length, followed by Dense layers leading to output.\n\nArguments\n\ninput_feature_length::Integer: length of feature vector at each node\nnum_conv::Integer: number of convolutional layers\nconv_activation::F: activation function on convolutional layers\natom_conv_feature_length::Integer: length of output of conv layers\npool_type::String: type of pooling after convolution (mean or max)\npool_width::Float: fraction of atomconvfeature_length that pooling window should span\npooled_feature_length::Integer: feature length to pool down to\nnum_hidden_layers::Integer: how many Dense layers before output? Note that if this is set to 1 there will be no nonlinearity imposed on these layers\nhidden_layer_activation::F: activation function on hidden layers\noutput_layer_activation::F: activation function on output layer; should generally be identity for regression and something that normalizes appropriately (e.g. softmax) for classification\noutput_length::Integer: length of output vector\ninitW::F: function to use to initialize weights in trainable layers\n\n\n\n\n\n","category":"function"},{"location":"comparison/#Comparison-of-AtomicGraphNets.jl-and-cgcnn.py","page":"Comparison with cgcnn.py","title":"Comparison of AtomicGraphNets.jl and cgcnn.py","text":"","category":"section"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"AtomicGraphNets.jl provides a model-builder for the typical GCNN architecture called Xie_model; in reference to Tian Xie, the original developer of cgcnn.py.\nHowever, there are some differences between how AtomicGraphNets models and those ones work, particularly with respect to the convolutional operation performed.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The cgcnn.py package was the first major package to implement atomic graph convolutional networks.\nHowever, the \"convolutional\" operation they use, while qualitatively similar, is not convolution by the strict definition involving the graph Laplacian. In their package, they introduce two such operations.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The first operation is expressed as follows.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nv^(t+1)_i = g big(sumlimits_jkv^(t)_j oplus u_(ij)_kbig)W^(t)_c + v^(t)_iW^(t)_s + b^(t)\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"Here, v, u represent node features and edge features respectively, and i, j, k index nodes, neighbors of nodes, and edge multiplicities respectively. Further, oplus indicates concatenation, and g is an activation function.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"Note that such an operation, which does not make use of the graph Laplacian, requires explicit computation of neighbor lists for every node, and that the convolutional weight matrix is of very large dimension due to the concatenation step.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The original CGCNN paper explores the following slightly more complicated operation that resulted in better performance.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nv^(t+1)_i = v^(t)_i + sumlimits_jksigma(z^(t)_(ij)_kW^(t)_f + b^(t)_f) odot g(z^(t)_(ij)_kW^(t)_s + b^(t)_s)\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"where z is a concatenation of neighbor features and edge features, and odot indicates element-wise multiplication.\nThis operation entails yet more trainable parameters, and neither operation is particularly performant because the concatenation operation must be done at each step of the forward pass.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"The operation implemented in AtomicGraphnets is as follows.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"beginaligned\nX^(t+1) = n_zbig(g(W^(t)_c cdot X^(t) cdot L + W^(t)_s cdot X^(t) + B^(t))big)\nendaligned","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"where X is a feature matrix constructed by stacking feature vectors, B is a bias matrix (stacked identical copies of the per-feature bias vector) and n_z is the z-score normalization (or regularized normalization operation), which we have found to improve stability.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"In addition, since the graph Laplacian need only be computed once (and is in fact stored as part of the AtomGraph type), the forward pass is much more computationally efficient. Since no concatenation occurs, weight matrices are also smaller, meaning the model has fewer trainable parameters, and no sacrifice in accuracy that we have been able to observe, indicating comparable expressivity.","category":"page"},{"location":"comparison/","page":"Comparison with cgcnn.py","title":"Comparison with cgcnn.py","text":"It is worth noting that one advantage of the cgcnn.py approach is that it allows for explicitly enumerating edge features. In the current version of AtomicGraphNets, the only features of graph edges are their weights. Convolutional operations that allow for edge features are under consideration for future versions.","category":"page"},{"location":"examples/example_1/#Materials-Project-Database","page":"Example 1","title":"Materials Project Database","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"In this example, we will create and train a neural network on the property formation_energy_per_atom based on the architecture as introduced in this paper, using the Materials Project database.","category":"page"},{"location":"examples/example_1/#.-Set-up-the-Dataset","page":"Example 1","title":"1. Set up the Dataset","text":"","category":"section"},{"location":"examples/example_1/#a.-Set-up-required-dependencies","page":"Example 1","title":"a. Set up required dependencies","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Create an API key with Materials Project to download the training dataset for this example.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Python packages (primarily pymatgen) are also required as dependencies. The easiest way to install these in a new environment, is using Conda, by running the following commands.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"conda create -name agn_example1\nconda activate agn_example1\nconda install -c conda-forge pymatgen=2022.0.4","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Note:  Alternatively, to install dependencies in an existing environment, you can skip the first step, activate your environment, and directly install pymatgen=2022.0.4.","category":"page"},{"location":"examples/example_1/#b.-Download-the-Data","page":"Example 1","title":"b. Download the Data","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"To download the structures, simply run","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"python download_data.py \"YOUR_API_KEY\"\nwhere, \"YOUR_API_KEY\" is replaced with your actual API key obtained from the Materials Project database.","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"This downloads the data required to train the models into the directory example1/data.","category":"page"},{"location":"examples/example_1/#.-Train-the-Network","page":"Example 1","title":"2. Train the Network","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Now that the dataset has been set up, run the formation_energy.jl file in your Julia environment and see what happens!","category":"page"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"Feel free to peruse and play with other options defined at the top of the file as well and see how it impacts the results!\nIn particular, the defaults have been set to only have a dataset of size 100 so that the base case will run quickly, but feel free to try more data to see how much better the results get.","category":"page"},{"location":"examples/example_1/#Note","page":"Example 1","title":"Note","text":"","category":"section"},{"location":"examples/example_1/","page":"Example 1","title":"Example 1","text":"By default, the property chosen for training is formation energy per atom. If you want to train a different property, replace it with its pymatgen string in the appropriate line. This change must be reflected in both download.py and example.jl.\nThere may be some CIF parsing warnings from pymatgen that show up, but these shouldn't affect things and can be safely ignored.","category":"page"},{"location":"graph_theory/#Basic-Graph-Theory","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A practical jumpstart, (and not an exhaustive resource), that introduces the bare minimum graph theory concepts to understand the math behind graph convolution.","category":"page"},{"location":"graph_theory/#What-is-a-graph?","page":"Basic Graph Theory","title":"What is a graph?","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A graph is an object described by a set of nodes and a set of edges that connect them:","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"(Image: Graph)","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"This is a useful data structure for all kinds of things, such as bridges in German cities (the historical origin of graph theory), social networks (in a more modern context), and many other applications. In this package, we're interested in using graphs to represent structures made of atoms such as crystals and molecules. Think of those ball-and-stick models of atoms and molecules from high school chemistry class!","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"In order to analyze graphs in a computer (for machine learning applications or otherwise), the first step is to assign an index to each node. It will turn out that for our purposes, it doesn't matter which node gets which label, which is very convenient!","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"(Image: Graph with node labels)","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Note that it is possible to have an edge with both ends connected to the same node (as shown in node 1 above), this is referred to as a self-loop.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The graph structure is represented on a computer using several types of matrices, defined below...","category":"page"},{"location":"graph_theory/#Adjacency-matrix","page":"Basic Graph Theory","title":"Adjacency matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Arguably, the most fundamental matrix representing a graph (because it has all the information required to construct the other two types we will introduce) is the adjacency matrix. An adjacency matrix is a square matrix of dimensions n times n, where n is the number of nodes in the graph being represented. The entry at index (i,j) in the adjacency matrix of an unweighted, undirected graph is 1 if node i and j are connected and zero otherwise.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"For our sample graph above, the adjacency matrix is:","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A = beginbmatrix\n1  1  1  0  0\n1  0  0  1  0\n1  0  0  1  0\n0  1  1  0  1\n0  0  0  1  0endbmatrix","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A few variations exist...","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"Weighted Graphs: Weighted graphs are graphs whose adjacency matrix representations have a continuum of values, rather than only 0s or 1s. AtomGraph objects as defined in ChemistryFeaturization.jl are weighted graphs, and the weights are scaled (without loss of generality) such that the largest weight is equal to 1.\nFor more on how weights are calculated, see the ChemistryFeaturization.jl documentation.\nDirected Graphs: A directed graph's adjacency matrix representation can be an asymmetric matrix. In other words, the edges of the graph have directionality. Atomic graphs are generally undirected.","category":"page"},{"location":"graph_theory/#Degree-matrix","page":"Basic Graph Theory","title":"Degree matrix","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The degree matrix of a graph is a diagonal matrix that describes how many edge terminations are at each node. For our sample graph, the degree matrix is given by:","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"D = beginbmatrix\n4  0  0  0  0\n0  2  0  0  0\n0  0  2  0  0\n0  0  0  3  0\n0  0  0  0  1endbmatrix","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"In a weighted graph, the degree matrix is constructed by summing the weights rather than just counting the nonzero entries in that row/column of the adjacency matrix.","category":"page"},{"location":"graph_theory/#Graph-Laplacian","page":"Basic Graph Theory","title":"Graph Laplacian","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The Laplacian matrix (also called the graph Laplacian) is defined as the difference of the degree and adjacency matrices.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"L = D - A = beginbmatrix\n3  -1  -1  0  0\n-1  2  0  -1  0\n-1  0  2  -1  0\n0  -1  -1  3  -1\n0  0  0  -1  1endbmatrix","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"As the name suggests, this matrix is closely tied to the differential operator that comes up in, for example, the diffusion equation. The graph laplacian matrix as an operator is, in fact, diffusional in nature. To get a better sense of this, let's define a simple graph signal, just a single number at each node of the graph, and we'll start with a \"delta spike\" on node 5 and zeros everywhere else. Watch what happens when we operate on that signal with the Laplacian a couple times...","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"(Image: Laplacian as an operator)","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"For a deeper dive into how graph Laplacian works, check this out. But at this point, the more important thing, is to know that this in the most generalized sense, is how we graph convolution is performed.","category":"page"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"In real applications, the \"graph signal\" is a feature matrix rather than a vector, as we'll generally have a vector of features for each node, and these get stacked to form the feature matrix. You can convince yourself with some basic linear algebra that the result of this is the same as if you applied the convolutional operation (i.e., multiplying by the Laplacian) to a bunch of vectors individually and stacked the results.","category":"page"},{"location":"graph_theory/#Remarks","page":"Basic Graph Theory","title":"Remarks","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A concise analysis that goes off on a tangent to discuss few facets that might be useful to understand before getting into graph convolution.","category":"page"},{"location":"graph_theory/#Equivariance-to-node-indexing","page":"Basic Graph Theory","title":"Equivariance to node indexing","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"The results of the convolution, i.e., the set of feature vectors at each node, are independent of the order in which the nodes are indexed, and identified. That is, if we relabel nodes with different indices, the adjacency, degree, Laplacian, and feature matrices are all re-calculated accordingly, in just the right way for everything to work out. Try out the example above with different node labels, to see this in practice.","category":"page"},{"location":"graph_theory/#Normalized-Laplacian","page":"Basic Graph Theory","title":"Normalized Laplacian","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"A not-so-nice aspect of the graph Laplacian as defined here, is that the magnitude of the graph signal can change upon repeated convolutions. In practice, the normalized Laplacian is used, which is computed using the inverse square root degree matrix. This helps regulate things better.","category":"page"},{"location":"graph_theory/#How-is-this-related-to-image-convolution?","page":"Basic Graph Theory","title":"How is this related to image convolution?","text":"","category":"section"},{"location":"graph_theory/","page":"Basic Graph Theory","title":"Basic Graph Theory","text":"For those familiar with image convolution (for those unfamiliar, there's a nice interactive tutorial here), it might not always be immediately obvious how this procedure is related. But, it turns out this is exactly the same thing. In typical image convolution, we've imposed a very rigid structure on the underlying graph namely that every node (pixel) has exactly the same sort of local structure (Cartesian neighbors). Graph convolution is more flexible, and allows for a more generalized notion of which \"pixels\" are \"neighbors.\"","category":"page"},{"location":"#AtomicGraphNets.jl","page":"Home","title":"AtomicGraphNets.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Graph based models for machine learning on atomic systems!","category":"page"},{"location":"","page":"Home","title":"Home","text":"AtomicGraphNets implements a variety of graph-based methods, such as Crystal Graph Convolutional Neural Nets; in Julia.\nIt makes use of the Flux ecosystem for model building, the JuliaGraphs ecosystem for graph representation, and ChemistryFeaturization for building, featurizing, and visualizing the graphs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in development as part of the ACED project, funded by ARPA-E DIFFERENTIATE and coordinated by Carnegie Mellon University, in collaboration with Julia Computing, Citrine Informatics, and MIT.","category":"page"},{"location":"gcnns/#Graph-Convolutional-Neural-Networks","page":"GCNNs","title":"Graph Convolutional Neural Networks","text":"","category":"section"},{"location":"gcnns/#Basic-Architecture","page":"GCNNs","title":"Basic Architecture","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"(Image: GCNN)","category":"page"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"This is the architecture of a typical graph convolutional neural network, or GCNN. There are three basic parts:","category":"page"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"Convolutional layers: any user-defined number of these in succession\nPooling layers: Because input graphs may have differing numbers of nodes, but we want eventual output to be a standardized length, we need a pooling procedure to get the variable-length output of the convolutional layers to a standardized length. Typically this is via some kind of moving filter which could take (for example) the maxima or averages over each window.\nDense layers: Usually, the last few layers of the network are \"standard\" densely-connected layers.","category":"page"},{"location":"gcnns/#Why-Graphs-CNNs?","page":"GCNNs","title":"Why Graphs + CNNs?","text":"","category":"section"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"Many machine learning architectures are constrained to working with 1-dimensional, regular, and uniform data structures. However, if presented with data represented in a structure (such as graphs) that doesn't quite fit this category, then typically the first step followed is to \"steamroll\" the data (using techniques such as dimensionality reduction) in an attempt to flatten it out, and essentially vectorize it. However, in doing so, vital spatial and structural aspects of the data are discarded. This is both philosophically unsatisfying and, pragmatically, can lead to spurious patterns being learned by models.","category":"page"},{"location":"gcnns/","page":"GCNNs","title":"GCNNs","text":"Structuring data as a graph and employing convolutional neural nets is an elegant and expressive solution to this conundrum. Graphs naturally encode connectedness between entities and convolutional kernels respect this locality in the way that information is propagated. ","category":"page"}]
}
